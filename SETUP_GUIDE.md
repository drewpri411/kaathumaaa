# Voice Agent - Quick Setup Guide

## âœ… Project Built Successfully!

All code components have been implemented. Follow these steps to get it running:

---

## ðŸ“‹ Step-by-Step Setup

### 1. Install Dependencies

```bash
cd voice-agent
python -m venv venv

# Activate virtual environment
venv\Scripts\activate  # Windows
# source venv/bin/activate  # macOS/Linux

pip install -r requirements.txt
```

### 2. Download Silero VAD Model

**Required**: Download the VAD model for voice activity detection

1. Visit: https://github.com/snakers4/silero-vad/releases
2. Download: `silero_vad.onnx` (latest version, ~2MB)
3. Place in: `voice-agent/models/silero_vad.onnx`

### 3. Configure API Keys

Copy `.env.example` to `.env` and add your keys:

```bash
copy .env.example .env
```

Edit `.env` file:
```env
OPENAI_API_KEY=sk-your-actual-key-here
```

### 4. Generate Backchannel Audio

You have **two options** for backchannel audio files:

#### Option A: Generate with OpenAI TTS (Recommended)

Uses OpenAI TTS to create natural-sounding backchannels. Requires OpenAI API key.

```bash
python generate_backchannels.py
```

This creates 5 WAV files in `backchannels/` directory:
- `mmhmm.wav` - Neutral acknowledgment
- `okay.wav` - Understanding
- `yeah.wav` - Agreement
- `i_see.wav` - Comprehension
- `right.wav` - Confirmation

**Cost**: ~$0.02 for all 5 files (one-time cost)

#### Option B: Use Placeholder Audio (No API Key Required)

If you don't have an OpenAI API key yet, you can create simple placeholder files:

```bash
python -c "import numpy as np, wave; [wave.open(f'backchannels/{name}.wav', 'w').setparams((1,2,16000,16000*0.3,'NONE','not compressed')) or wave.open(f'backchannels/{name}.wav', 'w').writeframes((np.sin(2*np.pi*440*np.linspace(0,0.3,int(16000*0.3)))*16000).astype(np.int16).tobytes()) for name in ['mmhmm','okay','yeah','i_see','right']]"
```

This creates simple tone placeholders. Replace with real audio later using Option A.

**Note**: The system will work with either option, but Option A provides much better user experience.

### 5. Start the Server

```bash
python -m server.main
```

Server starts on: http://localhost:8000

### 6. Test the Interface

1. Open browser: http://localhost:8000
2. Click "Start Conversation"
3. Grant microphone permission
4. Start speaking!

---

## ðŸ”‘ API Services Required

### OpenAI API Key
**Get it**: https://platform.openai.com/api-keys

**Used for**:
- **Whisper API**: Speech-to-Text transcription (chunked, 1.5s segments)
- **TTS API**: Text-to-Speech synthesis (alloy voice)
- **GPT-4o-mini**: LLM responses (concise, voice-optimized)
- **Backchannel generation**: Creating "mm-hmm", "okay", etc. audio files

**Cost estimate**: ~$0.01-0.05 per minute of conversation

---

## ðŸŽ¯ What to Test

Once running, test these scenarios:

1. **Short question**: "What's the weather like?"
   - Should respond quickly without interruption

2. **Long explanation**: Speak 3-4 sentences with natural pauses
   - Should hear backchannels ("mm-hmm", "okay") during pauses
   - Should NOT interrupt you mid-sentence

3. **List with pauses**: "I need to buy eggs, milk, and bread"
   - Should wait for complete thought before responding

4. **Interruption**: Start speaking while agent responds
   - Agent should stop and listen to you

---

## ðŸ”§ Troubleshooting

**"VAD model not found"**
â†’ Download silero_vad.onnx to models/ directory

**"Backchannel directory not found"**
â†’ Run `python generate_backchannels.py`

**"OpenAI API key not set"**
â†’ Add OPENAI_API_KEY to .env file

**"Gemini API key not set"**
â†’ Add GEMINI_API_KEY to .env file

**WebRTC connection fails**
â†’ Check firewall, try HTTPS, ensure microphone permission granted

---

## ðŸ“Š System Architecture

**Components Implemented** (20 files):
- âœ… Event bus system
- âœ… Configuration management
- âœ… Conversation state manager
- âœ… Audio pipeline with buffering
- âœ… VAD processor (Silero ONNX)
- âœ… STT client (Whisper API)
- âœ… Transcription coordinator with deduplication
- âœ… Linguistic analyzer
- âœ… Turn detector (multi-signal fusion)
- âœ… Backchannel system (5 components)
- âœ… LLM client (Gemini)
- âœ… TTS client (OpenAI)
- âœ… Response coordinator
- âœ… Audio mixer (multi-channel)
- âœ… WebRTC handler
- âœ… FastAPI server
- âœ… Web interface

**Turn Detection Algorithm**:
- 40% Silence duration (400ms-1500ms+ range)
- 35% Linguistic completeness (punctuation, grammar)
- 25% Conversation context (speaking duration, sentence count)
- Threshold: 65/100 for turn ending

**Backchannel System**:
- 5 types: mm-hmm, okay, yeah, I see, right
- 300ms safe zone with abort capability
- Context-aware selection
- Anti-repetition logic
- 40% base probability with modifiers

---

## ðŸš€ Next Steps After Setup

1. **Tune parameters**: Edit `.env` to adjust thresholds
2. **Monitor logs**: Watch console for turn detection scores
3. **Test edge cases**: Try different speaking patterns
4. **Adjust weights**: Modify SILENCE_WEIGHT, LINGUISTIC_WEIGHT, CONTEXT_WEIGHT
5. **Add more backchannels**: Edit generate_backchannels.py

See README.md for full documentation!
